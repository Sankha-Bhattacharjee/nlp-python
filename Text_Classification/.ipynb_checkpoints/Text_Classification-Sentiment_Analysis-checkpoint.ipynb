{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81ac03b",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df04ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9c71a",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8bcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_files('path_to_file txt_sentoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33bb0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reviews.data, reviews.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b76491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea7b73",
   "metadata": {},
   "source": [
    "### Persist the dataset in pickle files\n",
    "load_files method can take long time if we have lot of data in files, e.g. 50k data. So to quickly access the data we will store them in pickle files in bytes format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724fbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing as pickle files\n",
    "\n",
    "with open('X.pickle','wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('y.pickle','wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c9926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from pickle files\n",
    "del X\n",
    "del y\n",
    "\n",
    "with open('X.pickle','rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('y.pickle','rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8949f285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3f8f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\nparts of this are actually so absurd , that they would fit right in with dogma . \\nyes , the film is that weak , but it's better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\nanyway , this definitely doesn't seem like an arnold movie . \\nit just wasn't the type of film you can see him doing . \\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\nit's understandable , especially when the ending had to be changed according to some sources . \\naside form that , he still walked through it , much like he has in the past few films . \\ni'm sorry to say this arnold but maybe these are the end of your action days . \\nspeaking of action , where was it in this film ? \\nthere was hardly any explosions or fights . \\nthe devil made a few places explode , but arnold wasn't kicking some devil butt . \\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\ni also don't know why the film took so long and cost so much . \\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\nit's pitiful to see how scripts like these get bought and made into a movie . \\ndo they even read these things anymore ? \\nit sure doesn't seem like it . \\nthankfully gabriel's performance gave some light to this poor film . \\nwhen he walks down the street searching for robin tunney , you can't help but feel that he looked like a devil . \\nthe guy is creepy looking anyway ! \\nwhen it's all over , you're just glad it's the end of the movie . \\ndon't bother to see this , if you're expecting a solid action flick , because it's neither solid nor does it have action . \\nit's just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\nsave your money and see the world is not enough for an entertaining experience . \\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d7b34",
   "metadata": {},
   "source": [
    "### Creating Corpus\n",
    "Corpus is a list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969eade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(X)):\n",
    "    review = re.sub(r'\\W', ' ', str(X[i])) # remove all punctuations or not word characters\n",
    "    review = review.lower() # lowercase the string\n",
    "    review = re.sub(r'\\s+[a-z]\\s+', ' ', review) # remove single charactes (like i, a as they do not contribute to meaning much) from middle of the sentence, so we are replacting ' a ' by ' '\n",
    "    review = re.sub(r'^[a-z]\\s+', ' ', review) # remove single character from start of the sentence, so we are replacing 'i ' with ' '\n",
    "    review = re.sub(r'\\s+', ' ', review) # replace multiple spaces with a single space\n",
    "    review = review.strip() # remove extra spaces from start and end of the string\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac496df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80\\'s , but lately his films have been very sloppy and the one-liners are getting worse .  it\\'s hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what\\'s it matter to him ?  once again arnold has signed to do another expensive blockbuster , that can\\'t compare with the likes of the terminator series , true lies and even eraser .  in this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) .  with the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world !  parts of this are actually so absurd , that they would fit right in with dogma .  yes , the film is that weak , but it\\'s better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film .  anyway , this definitely doesn\\'t seem like an arnold movie .  it just wasn\\'t the type of film you can see him doing .  sure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going .  it\\'s understandable , especially when the ending had to be changed according to some sources .  aside form that , he still walked through it , much like he has in the past few films .  i\\'m sorry to say this arnold but maybe these are the end of your action days .  speaking of action , where was it in this film ?  there was hardly any explosions or fights .  the devil made a few places explode , but arnold wasn\\'t kicking some devil butt .  the ending was changed to make it more spiritual , which undoubtedly ruined the film .  i was at least hoping for a cool ending if nothing else occurred , but once again i was let down .  i also don\\'t know why the film took so long and cost so much .  there was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget .  the budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom .  it\\'s pitiful to see how scripts like these get bought and made into a movie .  do they even read these things anymore ?  it sure doesn\\'t seem like it .  thankfully gabriel\\'s performance gave some light to this poor film .  when he walks down the street searching for robin tunney , you can\\'t help but feel that he looked like a devil .  the guy is creepy looking anyway !  when it\\'s all over , you\\'re just glad it\\'s the end of the movie .  don\\'t bother to see this , if you\\'re expecting a solid action flick , because it\\'s neither solid nor does it have action .  it\\'s just another movie that we are suckered in to seeing , due to a strategic marketing campaign .  save your money and see the world is not enough for an entertaining experience .  \"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(str(X[0]).split('\\\\n')) # remove \\n from strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02feae72",
   "metadata": {},
   "source": [
    "### Create BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fae20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## max_features = selects top 2000 words from the histogram\n",
    "## min_df (minimum document frequency) = select words which is present in more than min_df documents.  \n",
    "##    Here we will words which are present in more than3 documents and discard words which are present 3 or less documents\n",
    "## max_df (maximum document frequency) = we will exclude all the words that appears >= max_df documnets\n",
    "##                                       represented as percentage. example: 'the', 'that'\n",
    "## stopwords = remove these stopwords\n",
    "\n",
    "## So vectorized will select words after applying min_df, max_df, stopwords and after that select top 2000 most frequent words\n",
    "vectorizer = CountVectorizer(max_features=2000, min_df=3, max_df=0.6, stop_words=stopwords.words('english'))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69552250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # (documents, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b88e5d",
   "metadata": {},
   "source": [
    "### Transform to TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac78cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X = transformer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f0b7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.06887219, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12007883, 0.        , 0.06321361, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a233e",
   "metadata": {},
   "source": [
    "### Split dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d61b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab6db25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 2000), (400, 2000), (1600,), (400,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef73bf2",
   "metadata": {},
   "source": [
    "### Build Logistic Regression Model for sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "015e2449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cbfe312",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ce3fe25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c74893f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[168,  40],\n",
       "       [ 21, 171]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cn = confusion_matrix(y_test, pred)\n",
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9cceaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb5cadbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85       208\n",
      "           1       0.81      0.89      0.85       192\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba93646",
   "metadata": {},
   "source": [
    "### Save our classifier and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "180f221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "357fea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.06887219, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12007883, 0.        , 0.06321361, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### save the vectprizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=3, max_df=0.6, stop_words=stopwords.words('english'))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0cc99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a3244",
   "metadata": {},
   "source": [
    "### Import classifier and vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff9fcfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "with open('vectorizer.pickle', 'rb') as f:\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15e4708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [\"You are a nice person man, have a good life.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f9e6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tfidf.transform(sample).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b52200d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db4edf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2=[\"You are a bad bad person.\"]\n",
    "sample2 = tfidf.transform(sample2).toarray()\n",
    "clf.predict(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
